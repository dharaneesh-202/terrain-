{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f1ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4c9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your training and testing datasets\n",
    "train_data_dir = 'C:/Users/User/Downloads/terrain classification/dataset-1/train'\n",
    "test_data_dir = 'C:/Users/User/Downloads/terrain classification/dataset-1/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814966c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "input_shape = (224, 224, 3)  # Adjust dimensions as per your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b34bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for the training dataset (optional but recommended)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745b0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1813 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Change to 'binary' if it's binary classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f11078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 778 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the testing dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd54843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))  # Adjust num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14b31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ff700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "30/57 [==============>...............] - ETA: 1:10 - loss: 1.4589 - accuracy: 0.5375"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=30,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5de52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\New folder\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save your trained model to a file\n",
    "model.save('my_model.h5')  # Replace 'my_model.h5' with your desired file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e001b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model.h5')  # Replace with the path to your saved model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6833961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the new image you want to classify\n",
    "new_image_path = 'C:/Users/User/Desktop/preview16.jpg'  # Replace with the path to your image\n",
    "new_image = image.load_img(new_image_path, target_size=(224, 224))  # Adjust target_size as needed\n",
    "new_image = image.img_to_array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "new_image = new_image / 255.0  # Normalize the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb6a8e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the loaded model to predict the class of the new image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(new_image)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get the predicted class label (assuming one-hot encoding is used)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m predicted_class_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to predict the class of the new image\n",
    "predictions = model.predict(new_image)\n",
    "\n",
    "# Get the predicted class label (assuming one-hot encoding is used)\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# You may have a mapping of class indices to class names\n",
    "class_names = {0: 'grassy', 1: 'marshy', 2: 'rocky', 3: 'sandy'}  # Update with your class names\n",
    "\n",
    "# Get the predicted class name\n",
    "predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d321a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "class_indices = np.argmax(predictions, axis=1)\n",
    "print (class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84273072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.9870713e-11 1.0000000e+00 5.6010228e-09 3.1583505e-10]]\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65c380af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 778 images belonging to 4 classes.\n",
      "25/25 [==============================] - 15s 587ms/step\n",
      "Confusion Matrix:\n",
      "[[ 10  31  22  28]\n",
      " [ 24  37  46  63]\n",
      " [ 24  47  51  61]\n",
      " [ 42  71  85 136]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      grassy       0.10      0.11      0.10        91\n",
      "      marshy       0.20      0.22      0.21       170\n",
      "       rocky       0.25      0.28      0.26       183\n",
      "       sandy       0.47      0.41      0.44       334\n",
      "\n",
      "    accuracy                           0.30       778\n",
      "   macro avg       0.26      0.25      0.25       778\n",
      "weighted avg       0.32      0.30      0.31       778\n",
      "\n",
      "\n",
      "Accuracy: 0.30077120822622105\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load your test dataset\n",
    "test_data_dir = 'C:/Users/User/Downloads/terrain classification/dataset-1/test'  # Replace with the path to your test dataset\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)  # Make sure to use the same preprocessing as during training\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=input_shape[:2],  # Use the same target size as during training\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Change to 'binary' if it's binary classification\n",
    ")\n",
    "\n",
    "# Predict the classes for the test dataset\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true classes from the test dataset\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Calculate confusion matrix and other metrics\n",
    "confusion = confusion_matrix(true_classes, predicted_classes)\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_names.values())\n",
    "\n",
    "# Print the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)\n",
    "print(\"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ecf0e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy Over Epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the 'history' object from your model training\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
