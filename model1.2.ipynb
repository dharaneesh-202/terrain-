{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f1ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4c9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your training and testing datasets\n",
    "train_data_dir = 'C:/Users/User/Downloads/terrain classification/dataset-1/train'\n",
    "test_data_dir = 'C:/Users/User/Downloads/terrain classification/dataset-1/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814966c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "input_shape = (224, 224, 3)  # Adjust dimensions as per your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b34bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for the training dataset (optional but recommended)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745b0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1813 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Change to 'binary' if it's binary classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f11078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 778 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the testing dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd54843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))  # Adjust num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f14b31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6ff700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "57/57 [==============================] - 130s 2s/step - loss: 0.8327 - accuracy: 0.6895 - val_loss: 0.5026 - val_accuracy: 0.8406\n",
      "Epoch 2/35\n",
      "57/57 [==============================] - 147s 3s/step - loss: 0.5229 - accuracy: 0.7882 - val_loss: 0.5163 - val_accuracy: 0.8201\n",
      "Epoch 3/35\n",
      "57/57 [==============================] - 148s 3s/step - loss: 0.4827 - accuracy: 0.8229 - val_loss: 0.5144 - val_accuracy: 0.8316\n",
      "Epoch 4/35\n",
      "57/57 [==============================] - 137s 2s/step - loss: 0.4458 - accuracy: 0.8367 - val_loss: 0.5204 - val_accuracy: 0.7995\n",
      "Epoch 5/35\n",
      "57/57 [==============================] - 136s 2s/step - loss: 0.3868 - accuracy: 0.8693 - val_loss: 0.4300 - val_accuracy: 0.8509\n",
      "Epoch 6/35\n",
      "57/57 [==============================] - 122s 2s/step - loss: 0.3659 - accuracy: 0.8649 - val_loss: 0.5109 - val_accuracy: 0.8316\n",
      "Epoch 7/35\n",
      "57/57 [==============================] - 122s 2s/step - loss: 0.3903 - accuracy: 0.8566 - val_loss: 0.3808 - val_accuracy: 0.8753\n",
      "Epoch 8/35\n",
      "57/57 [==============================] - 122s 2s/step - loss: 0.3543 - accuracy: 0.8715 - val_loss: 0.4288 - val_accuracy: 0.8612\n",
      "Epoch 9/35\n",
      "57/57 [==============================] - 122s 2s/step - loss: 0.3333 - accuracy: 0.8742 - val_loss: 0.5102 - val_accuracy: 0.8393\n",
      "Epoch 10/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.3207 - accuracy: 0.8825 - val_loss: 0.3749 - val_accuracy: 0.8728\n",
      "Epoch 11/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.3561 - accuracy: 0.8753 - val_loss: 0.4593 - val_accuracy: 0.8419\n",
      "Epoch 12/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.3028 - accuracy: 0.8924 - val_loss: 0.3369 - val_accuracy: 0.8792\n",
      "Epoch 13/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.3089 - accuracy: 0.8820 - val_loss: 0.3248 - val_accuracy: 0.8843\n",
      "Epoch 14/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2674 - accuracy: 0.9029 - val_loss: 0.3857 - val_accuracy: 0.8817\n",
      "Epoch 15/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2764 - accuracy: 0.9035 - val_loss: 0.4345 - val_accuracy: 0.8740\n",
      "Epoch 16/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.3168 - accuracy: 0.8798 - val_loss: 0.4496 - val_accuracy: 0.8483\n",
      "Epoch 17/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.3162 - accuracy: 0.8908 - val_loss: 0.3965 - val_accuracy: 0.8638\n",
      "Epoch 18/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2847 - accuracy: 0.8902 - val_loss: 0.3960 - val_accuracy: 0.8715\n",
      "Epoch 19/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2819 - accuracy: 0.8958 - val_loss: 0.2629 - val_accuracy: 0.9126\n",
      "Epoch 20/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2512 - accuracy: 0.9106 - val_loss: 0.2747 - val_accuracy: 0.9126\n",
      "Epoch 21/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.2694 - accuracy: 0.8946 - val_loss: 0.2908 - val_accuracy: 0.9087\n",
      "Epoch 22/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.2525 - accuracy: 0.9095 - val_loss: 0.2708 - val_accuracy: 0.9087\n",
      "Epoch 23/35\n",
      "57/57 [==============================] - 120s 2s/step - loss: 0.2536 - accuracy: 0.9101 - val_loss: 0.3172 - val_accuracy: 0.9010\n",
      "Epoch 24/35\n",
      "57/57 [==============================] - 129s 2s/step - loss: 0.2330 - accuracy: 0.9178 - val_loss: 0.2882 - val_accuracy: 0.9087\n",
      "Epoch 25/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2306 - accuracy: 0.9129 - val_loss: 0.2721 - val_accuracy: 0.9113\n",
      "Epoch 26/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2486 - accuracy: 0.9123 - val_loss: 0.2651 - val_accuracy: 0.9216\n",
      "Epoch 27/35\n",
      "57/57 [==============================] - 121s 2s/step - loss: 0.2235 - accuracy: 0.9156 - val_loss: 0.2873 - val_accuracy: 0.9075\n",
      "Epoch 28/35\n",
      "57/57 [==============================] - 123s 2s/step - loss: 0.2323 - accuracy: 0.9129 - val_loss: 0.3224 - val_accuracy: 0.9010\n",
      "Epoch 29/35\n",
      "57/57 [==============================] - 122s 2s/step - loss: 0.2237 - accuracy: 0.9184 - val_loss: 0.2935 - val_accuracy: 0.9036\n",
      "Epoch 30/35\n",
      "57/57 [==============================] - 122s 2s/step - loss: 0.2574 - accuracy: 0.9040 - val_loss: 0.3062 - val_accuracy: 0.8985\n",
      "Epoch 31/35\n",
      "57/57 [==============================] - 131s 2s/step - loss: 0.2269 - accuracy: 0.9129 - val_loss: 0.2563 - val_accuracy: 0.9113\n",
      "Epoch 32/35\n",
      "57/57 [==============================] - 155s 3s/step - loss: 0.2354 - accuracy: 0.9068 - val_loss: 0.3052 - val_accuracy: 0.9036\n",
      "Epoch 33/35\n",
      "57/57 [==============================] - 145s 3s/step - loss: 0.2138 - accuracy: 0.9250 - val_loss: 0.2832 - val_accuracy: 0.9165\n",
      "Epoch 34/35\n",
      "57/57 [==============================] - 139s 2s/step - loss: 0.2118 - accuracy: 0.9200 - val_loss: 0.2676 - val_accuracy: 0.9152\n",
      "Epoch 35/35\n",
      "57/57 [==============================] - 139s 2s/step - loss: 0.2089 - accuracy: 0.9288 - val_loss: 0.2584 - val_accuracy: 0.9177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f70673d570>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=35,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3583927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 11s 445ms/step - loss: 0.2584 - accuracy: 0.9177\n",
      "Test accuracy: 0.9177377820014954\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5de52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\New folder\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save your trained model to a file\n",
    "model.save('my_model.h5')  # Replace 'my_model.h5' with your desired file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e001b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model.h5')  # Replace with the path to your saved model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6833961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the new image you want to classify\n",
    "new_image_path = 'C:/Users/User/Desktop/sandy.jpg'  # Replace with the path to your image\n",
    "new_image = image.load_img(new_image_path, target_size=(224, 224))  # Adjust target_size as needed\n",
    "new_image = image.img_to_array(new_image)\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "new_image = new_image / 255.0  # Normalize the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbb6a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted class: sandy\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to predict the class of the new image\n",
    "predictions = model.predict(new_image)\n",
    "\n",
    "# Get the predicted class label (assuming one-hot encoding is used)\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# You may have a mapping of class indices to class names\n",
    "class_names = {0: 'grassy', 1: 'marshy', 2: 'rocky', 3: 'sandy'}  # Update with your class names\n",
    "\n",
    "# Get the predicted class name\n",
    "predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d321a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "class_indices = np.argmax(predictions, axis=1)\n",
    "print (class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84273072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.5955539e-01 4.0443756e-02 8.4028915e-07 3.8132231e-09]]\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65c380af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 778 images belonging to 4 classes.\n",
      "25/25 [==============================] - 13s 501ms/step\n",
      "Confusion Matrix:\n",
      "[[  9  28  20  34]\n",
      " [ 17  47  38  68]\n",
      " [ 20  41  45  77]\n",
      " [ 34  74  87 139]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      grassy       0.11      0.10      0.11        91\n",
      "      marshy       0.25      0.28      0.26       170\n",
      "       rocky       0.24      0.25      0.24       183\n",
      "       sandy       0.44      0.42      0.43       334\n",
      "\n",
      "    accuracy                           0.31       778\n",
      "   macro avg       0.26      0.26      0.26       778\n",
      "weighted avg       0.31      0.31      0.31       778\n",
      "\n",
      "\n",
      "Accuracy: 0.30848329048843187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load your test dataset\n",
    "test_data_dir = 'C:/Users/User/Downloads/terrain classification/dataset-1/test'  # Replace with the path to your test dataset\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)  # Make sure to use the same preprocessing as during training\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=input_shape[:2],  # Use the same target size as during training\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Change to 'binary' if it's binary classification\n",
    ")\n",
    "\n",
    "# Predict the classes for the test dataset\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true classes from the test dataset\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Calculate confusion matrix and other metrics\n",
    "confusion = confusion_matrix(true_classes, predicted_classes)\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_names.values())\n",
    "\n",
    "# Print the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)\n",
    "print(\"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ecf0e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy Over Epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAGyCAYAAAA70Ml8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa7ElEQVR4nO3dbWxUZd7H8d+0pVNkt2MELS3UWlzQKhGXNlTKNkYXa4BgSNxQ48aii4mNuhW6sFK7ASEmjW4kK0rrUwsxKWyDiuFFV5kXu1Ae9oFua4xtorGsLdrStMZpBbdAue4XLJN7bKucofMvrd9PMi/m4pyZa64058s5nen4nHNOAAAYihvrCQAAfnyIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwJzn+Bw8eFDLly9XWlqafD6f3nvvvR/c58CBA8rOzlZSUpJmzZqlV199NZq5AgAmCM/xOXXqlObNm6dXXnnlkrY/fvy4li5dqvz8fDU1NemZZ55RSUmJ3nnnHc+TBQBMDL7L+cOiPp9Pe/fu1YoVK0bc5umnn9a+ffvU2toaHisuLtaHH36oo0ePRvvUAIBxLCHWT3D06FEVFBREjN17772qrq7W2bNnNWnSpCH7DAwMaGBgIHz//Pnz+uqrrzR16lT5fL5YTxkA8D/OOfX39ystLU1xcaP3NoGYx6erq0spKSkRYykpKTp37px6enqUmpo6ZJ+Kigpt3rw51lMDAFyijo4OzZw5c9QeL+bxkTTkbOXilb6RzmLKyspUWloavh8KhXT99dero6NDycnJsZsoACBCX1+f0tPT9dOf/nRUHzfm8Zk+fbq6uroixrq7u5WQkKCpU6cOu4/f75ff7x8ynpycTHwAYAyM9q88Yv45n4ULFyoYDEaM7d+/Xzk5OcP+vgcAMPF5js8333yj5uZmNTc3S7rwVurm5ma1t7dLunDJrKioKLx9cXGxPv/8c5WWlqq1tVU1NTWqrq7WunXrRucVAADGHc+X3Y4dO6a77rorfP/i72ZWrVqlnTt3qrOzMxwiScrMzFR9fb3Wrl2r7du3Ky0tTdu2bdP9998/CtMHAIxHl/U5Hyt9fX0KBAIKhUL8zgcADMXq+MvfdgMAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHNRxaeyslKZmZlKSkpSdna2Ghoavnf72tpazZs3T1dddZVSU1P1yCOPqLe3N6oJAwDGP8/xqaur05o1a1ReXq6mpibl5+dryZIlam9vH3b7Q4cOqaioSKtXr9bHH3+sPXv26F//+pceffTRy548AGB88hyfrVu3avXq1Xr00UeVlZWlP/3pT0pPT1dVVdWw2//973/XDTfcoJKSEmVmZuoXv/iFHnvsMR07duyyJw8AGJ88xefMmTNqbGxUQUFBxHhBQYGOHDky7D55eXk6ceKE6uvr5ZzTyZMn9fbbb2vZsmUjPs/AwID6+voibgCAicNTfHp6ejQ4OKiUlJSI8ZSUFHV1dQ27T15enmpra1VYWKjExERNnz5dV199tV5++eURn6eiokKBQCB8S09P9zJNAMAVLqo3HPh8voj7zrkhYxe1tLSopKREGzduVGNjo95//30dP35cxcXFIz5+WVmZQqFQ+NbR0RHNNAEAV6gELxtPmzZN8fHxQ85yuru7h5wNXVRRUaFFixZp/fr1kqTbbrtNU6ZMUX5+vp577jmlpqYO2cfv98vv93uZGgBgHPF05pOYmKjs7GwFg8GI8WAwqLy8vGH3OX36tOLiIp8mPj5e0oUzJgDAj4/ny26lpaV68803VVNTo9bWVq1du1bt7e3hy2hlZWUqKioKb798+XK9++67qqqqUltbmw4fPqySkhItWLBAaWlpo/dKAADjhqfLbpJUWFio3t5ebdmyRZ2dnZo7d67q6+uVkZEhSers7Iz4zM/DDz+s/v5+vfLKK/rd736nq6++Wnfffbeef/750XsVAIBxxefGwbWvvr4+BQIBhUIhJScnj/V0AOBHI1bHX/62GwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmIsqPpWVlcrMzFRSUpKys7PV0NDwvdsPDAyovLxcGRkZ8vv9uvHGG1VTUxPVhAEA41+C1x3q6uq0Zs0aVVZWatGiRXrttde0ZMkStbS06Prrrx92n5UrV+rkyZOqrq7Wz372M3V3d+vcuXOXPXkAwPjkc845Lzvk5uZq/vz5qqqqCo9lZWVpxYoVqqioGLL9+++/rwceeEBtbW265pproppkX1+fAoGAQqGQkpOTo3oMAIB3sTr+errsdubMGTU2NqqgoCBivKCgQEeOHBl2n3379iknJ0cvvPCCZsyYoTlz5mjdunX69ttvR3yegYEB9fX1RdwAABOHp8tuPT09GhwcVEpKSsR4SkqKurq6ht2nra1Nhw4dUlJSkvbu3auenh49/vjj+uqrr0b8vU9FRYU2b97sZWoAgHEkqjcc+Hy+iPvOuSFjF50/f14+n0+1tbVasGCBli5dqq1bt2rnzp0jnv2UlZUpFAqFbx0dHdFMEwBwhfJ05jNt2jTFx8cPOcvp7u4ecjZ0UWpqqmbMmKFAIBAey8rKknNOJ06c0OzZs4fs4/f75ff7vUwNADCOeDrzSUxMVHZ2toLBYMR4MBhUXl7esPssWrRIX375pb755pvw2CeffKK4uDjNnDkziikDAMY7z5fdSktL9eabb6qmpkatra1au3at2tvbVVxcLOnCJbOioqLw9g8++KCmTp2qRx55RC0tLTp48KDWr1+v3/zmN5o8efLovRIAwLjh+XM+hYWF6u3t1ZYtW9TZ2am5c+eqvr5eGRkZkqTOzk61t7eHt//JT36iYDCo3/72t8rJydHUqVO1cuVKPffcc6P3KgAA44rnz/mMBT7nAwBj44r4nA8AAKOB+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMRRWfyspKZWZmKikpSdnZ2WpoaLik/Q4fPqyEhATdfvvt0TwtAGCC8Byfuro6rVmzRuXl5WpqalJ+fr6WLFmi9vb2790vFAqpqKhIv/zlL6OeLABgYvA555yXHXJzczV//nxVVVWFx7KysrRixQpVVFSMuN8DDzyg2bNnKz4+Xu+9956am5sv+Tn7+voUCAQUCoWUnJzsZboAgMsQq+OvpzOfM2fOqLGxUQUFBRHjBQUFOnLkyIj77dixQ5999pk2bdp0Sc8zMDCgvr6+iBsAYOLwFJ+enh4NDg4qJSUlYjwlJUVdXV3D7vPpp59qw4YNqq2tVUJCwiU9T0VFhQKBQPiWnp7uZZoAgCtcVG848Pl8Efedc0PGJGlwcFAPPvigNm/erDlz5lzy45eVlSkUCoVvHR0d0UwTAHCFurRTkf+ZNm2a4uPjh5zldHd3DzkbkqT+/n4dO3ZMTU1NevLJJyVJ58+fl3NOCQkJ2r9/v+6+++4h+/n9fvn9fi9TAwCMI57OfBITE5Wdna1gMBgxHgwGlZeXN2T75ORkffTRR2pubg7fiouLddNNN6m5uVm5ubmXN3sAwLjk6cxHkkpLS/XQQw8pJydHCxcu1Ouvv6729nYVFxdLunDJ7IsvvtBbb72luLg4zZ07N2L/6667TklJSUPGAQA/Hp7jU1hYqN7eXm3ZskWdnZ2aO3eu6uvrlZGRIUnq7Oz8wc/8AAB+3Dx/zmcs8DkfABgbV8TnfAAAGA3EBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGAuqvhUVlYqMzNTSUlJys7OVkNDw4jbvvvuu7rnnnt07bXXKjk5WQsXLtQHH3wQ9YQBAOOf5/jU1dVpzZo1Ki8vV1NTk/Lz87VkyRK1t7cPu/3Bgwd1zz33qL6+Xo2Njbrrrru0fPlyNTU1XfbkAQDjk88557zskJubq/nz56uqqio8lpWVpRUrVqiiouKSHuPWW29VYWGhNm7ceEnb9/X1KRAIKBQKKTk52ct0AQCXIVbHX09nPmfOnFFjY6MKCgoixgsKCnTkyJFLeozz58+rv79f11xzzYjbDAwMqK+vL+IGAJg4PMWnp6dHg4ODSklJiRhPSUlRV1fXJT3Giy++qFOnTmnlypUjblNRUaFAIBC+paene5kmAOAKF9UbDnw+X8R959yQseHs3r1bzz77rOrq6nTdddeNuF1ZWZlCoVD41tHREc00AQBXqAQvG0+bNk3x8fFDznK6u7uHnA19V11dnVavXq09e/Zo8eLF37ut3++X3+/3MjUAwDji6cwnMTFR2dnZCgaDEePBYFB5eXkj7rd79249/PDD2rVrl5YtWxbdTAEAE4anMx9JKi0t1UMPPaScnBwtXLhQr7/+utrb21VcXCzpwiWzL774Qm+99ZakC+EpKirSSy+9pDvuuCN81jR58mQFAoFRfCkAgPHCc3wKCwvV29urLVu2qLOzU3PnzlV9fb0yMjIkSZ2dnRGf+Xnttdd07tw5PfHEE3riiSfC46tWrdLOnTsv/xUAAMYdz5/zGQt8zgcAxsYV8TkfAABGA/EBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmCM+AABzxAcAYI74AADMER8AgDniAwAwR3wAAOaIDwDAHPEBAJgjPgAAc8QHAGCO+AAAzBEfAIA54gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMAc8QEAmIsqPpWVlcrMzFRSUpKys7PV0NDwvdsfOHBA2dnZSkpK0qxZs/Tqq69GNVkAwMTgOT51dXVas2aNysvL1dTUpPz8fC1ZskTt7e3Dbn/8+HEtXbpU+fn5ampq0jPPPKOSkhK98847lz15AMD45HPOOS875Obmav78+aqqqgqPZWVlacWKFaqoqBiy/dNPP619+/aptbU1PFZcXKwPP/xQR48evaTn7OvrUyAQUCgUUnJyspfpAgAuQ6yOvwleNj5z5owaGxu1YcOGiPGCggIdOXJk2H2OHj2qgoKCiLF7771X1dXVOnv2rCZNmjRkn4GBAQ0MDITvh0IhSRcWAQBg5+Jx1+N5yg/yFJ+enh4NDg4qJSUlYjwlJUVdXV3D7tPV1TXs9ufOnVNPT49SU1OH7FNRUaHNmzcPGU9PT/cyXQDAKOnt7VUgEBi1x/MUn4t8Pl/EfefckLEf2n648YvKyspUWloavv/1118rIyND7e3to/rix7u+vj6lp6ero6ODy5HfwdoMj3UZGWszvFAopOuvv17XXHPNqD6up/hMmzZN8fHxQ85yuru7h5zdXDR9+vRht09ISNDUqVOH3cfv98vv9w8ZDwQC/FAMIzk5mXUZAWszPNZlZKzN8OLiRveTOZ4eLTExUdnZ2QoGgxHjwWBQeXl5w+6zcOHCIdvv379fOTk5w/6+BwAw8XlOWWlpqd58803V1NSotbVVa9euVXt7u4qLiyVduGRWVFQU3r64uFiff/65SktL1draqpqaGlVXV2vdunWj9yoAAOOK59/5FBYWqre3V1u2bFFnZ6fmzp2r+vp6ZWRkSJI6OzsjPvOTmZmp+vp6rV27Vtu3b1daWpq2bdum+++//5Kf0+/3a9OmTcNeivsxY11GxtoMj3UZGWszvFiti+fP+QAAcLn4224AAHPEBwBgjvgAAMwRHwCAuSsmPnxNw/C8rMu7776re+65R9dee62Sk5O1cOFCffDBB4azteX1Z+aiw4cPKyEhQbfffntsJzhGvK7LwMCAysvLlZGRIb/frxtvvFE1NTVGs7XjdV1qa2s1b948XXXVVUpNTdUjjzyi3t5eo9naOXjwoJYvX660tDT5fD699957P7jPqBx/3RXgz3/+s5s0aZJ74403XEtLi3vqqafclClT3Oeffz7s9m1tbe6qq65yTz31lGtpaXFvvPGGmzRpknv77beNZx5bXtflqaeecs8//7z75z//6T755BNXVlbmJk2a5P79738bzzz2vK7NRV9//bWbNWuWKygocPPmzbOZrKFo1uW+++5zubm5LhgMuuPHj7t//OMf7vDhw4azjj2v69LQ0ODi4uLcSy+95Nra2lxDQ4O79dZb3YoVK4xnHnv19fWuvLzcvfPOO06S27t37/duP1rH3ysiPgsWLHDFxcURYzfffLPbsGHDsNv//ve/dzfffHPE2GOPPebuuOOOmM1xLHhdl+HccsstbvPmzaM9tTEX7doUFha6P/zhD27Tpk0TMj5e1+Uvf/mLCwQCrre312J6Y8bruvzxj390s2bNihjbtm2bmzlzZszmeCW4lPiM1vF3zC+7Xfyahu9+7UI0X9Nw7NgxnT17NmZztRTNunzX+fPn1d/fP+p/EHCsRbs2O3bs0GeffaZNmzbFeopjIpp12bdvn3JycvTCCy9oxowZmjNnjtatW6dvv/3WYsomolmXvLw8nThxQvX19XLO6eTJk3r77be1bNkyiylf0Ubr+BvVX7UeTVZf0zDeRLMu3/Xiiy/q1KlTWrlyZSymOGaiWZtPP/1UGzZsUENDgxISxvzHPiaiWZe2tjYdOnRISUlJ2rt3r3p6evT444/rq6++mjC/94lmXfLy8lRbW6vCwkL997//1blz53Tffffp5ZdftpjyFW20jr9jfuZzUay/pmG88rouF+3evVvPPvus6urqdN1118VqemPqUtdmcHBQDz74oDZv3qw5c+ZYTW/MePmZOX/+vHw+n2pra7VgwQItXbpUW7du1c6dOyfU2Y/kbV1aWlpUUlKijRs3qrGxUe+//76OHz8e/huWP3ajcfwd8/8CWn1Nw3gTzbpcVFdXp9WrV2vPnj1avHhxLKc5JryuTX9/v44dO6ampiY9+eSTki4cdJ1zSkhI0P79+3X33XebzD2WovmZSU1N1YwZMyK+JysrK0vOOZ04cUKzZ8+O6ZwtRLMuFRUVWrRokdavXy9Juu222zRlyhTl5+frueeemxBXV6I1WsffMT/z4WsahhfNukgXzngefvhh7dq1a8Jen/a6NsnJyfroo4/U3NwcvhUXF+umm25Sc3OzcnNzraYeU9H8zCxatEhffvmlvvnmm/DYJ598ori4OM2cOTOm87USzbqcPn16yPfXxMfHSxr9r5Meb0bt+Ovp7QkxcvFtkNXV1a6lpcWtWbPGTZkyxf3nP/9xzjm3YcMG99BDD4W3v/hWv7Vr17qWlhZXXV09od9qfanrsmvXLpeQkOC2b9/uOjs7w7evv/56rF5CzHhdm++aqO9287ou/f39bubMme5Xv/qV+/jjj92BAwfc7Nmz3aOPPjpWLyEmvK7Ljh07XEJCgqusrHSfffaZO3TokMvJyXELFiwYq5cQM/39/a6pqck1NTU5SW7r1q2uqakp/Db0WB1/r4j4OOfc9u3bXUZGhktMTHTz5893Bw4cCP/bqlWr3J133hmx/d/+9jf385//3CUmJrobbrjBVVVVGc/Yhpd1ufPOO52kIbdVq1bZT9yA15+Z/2+ixsc57+vS2trqFi9e7CZPnuxmzpzpSktL3enTp41nHXte12Xbtm3ulltucZMnT3apqanu17/+tTtx4oTxrGPvr3/96/ceN2J1/OUrFQAA5sb8dz4AgB8f4gMAMEd8AADmiA8AwBzxAQCYIz4AAHPEBwBgjvgAAMwRHwCAOeIDADBHfAAA5ogPAMDc/wG1ASdqMbroQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the 'history' object from your model training\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
